Instructions for "AWS Production EC2 and S3 cloned from local PC (no processing)"
================================================================================

This manual is intended for transferring the outputs of the update process on a local PC to cloud-based services AWS EC2 and S3.
It is assumed that the following guides were completed:
a. "AWS version setup for production only.txt" (or "AWS version setup for processing and production.txt")
b. "Local version setup for processing and production.txt"
c. "Instructions for Monthly auto update on local pc.txt" or 
   "Instructions for On demand date on local pc no TTM.txt" or
   "Instructions for On demand date on local pc.txt"
   
1. Upload the HTML and static data files to S3
==============================================

1.1 - Monthly update files
1.2 - On-demand files

1.1 - Monthly update files
--------------------------

You should have 3 buckets available to host on the S3 service:
<some-prefix>-current, e.g. transitanalystisrael-current
<some-prefix>-past, e.g. transitanalystisrael-past
<some-prefix>-backup, e.g. transitanalystisrael-backup

a. Updating or Valdiating Transit Analyst Israel files are up-to-date with the bucket names:
   - Open TransitAnalystIsrael/root/transitisrael_config.py
   - Update the "bucket_prefix" parameter with <some-prefix> if needed.
   - Save and exit

b. Updating or Valdiating Transit Analyst Israel files are up-to-date with the EC2 API Gateway URL
The static HTML files should have the correct API Getway URL that points to the AWS EC2 before being uplaoded to the S3.
a. Go to https://eu-central-1.console.aws.amazon.com/apigateway/
b.1. Click on the API Gateway you defined earlier -> Click "Stages" -> Click on the staging name -> Copy the invoke URL
b.2. Make sure that that parameter time_map_server_aws_url value is the same as the copied invoke URL in the following locations on your local PC:
	 - TransitAnalystIsrael/root/transitisrael_config.py
	 - TransitAnalystIsrael/website_current/transitisrael_config.js
	 - TransitAnalystIsrael/website_past/transitisrael_config.js

c. Performing the upload
Now you will upload the curent and past website files to S3. 
c.1. Go to TransitAnalystIsrael/root and run upload_current2aws_s3.py
c.2. Go to TransitAnalystIsrael/root and run upload_past2aws_s3.py
c.3. Once finished, 
		the current period files should be available at https://s3.console.aws.amazon.com/s3/buckets/<<some-prefix>-current>/
		the past period files should be available at https://s3.console.aws.amazon.com/s3/buckets/<<some-prefix>-past>/

d. Update and upload to S3 config files to make the TTM client look for the TTM server on EC2 and not on the local PC. We do this by changing the config to look like the product processed is on EC2 and S3.
d.1. copy website_current\docs\transitanalystisrael_config.js to a temp folder e.g. C:\temp\current
d.2. copy website_past\docs\transitanalystisrael_config.js to a temp folder e.g. C:\temp\past
d.3. edit transitanalystisrael_config.js in both current and past temp folders. 
		Replace the following 13 lines:
// #Monthly auto update on AWS EC2 and S3
// #get_service_date = 'auto'
// #python_processing = 'aws_ec2'
// #ttm_graph_processing = 'aws_ec2'
// #web_client_hosted_on = 'aws_s3'
// #ttm_server_on = 'aws_ec2'

// #Monthly auto update on local pc
var cfg_get_service_date = 'auto' ;
var cfg_python_processing = 'local_pc' ;
var cfg_ttm_graph_processing = 'local_pc' ;
var cfg_web_client_hosted_on = 'local_pc' ;
var cfg_ttm_server_on = 'local_pc' ;

		With the following 13 lines:
// #Monthly auto update on AWS EC2 and S3
var cfg_get_service_date = 'auto' ;
var cfg_python_processing = 'aws_ec2' ;
var cfg_ttm_graph_processing = 'aws_ec2' ;
var cfg_web_client_hosted_on = 'aws_s3' ;
var cfg_ttm_server_on = 'aws_ec2' ;

// #Monthly auto update on local pc
// #get_service_date = 'auto'
// #python_processing = 'local_pc'
// #ttm_graph_processing = 'local_pc'
// #web_client_hosted_on = 'local_pc'
// #ttm_server_on = 'local_pc'

d.4. use the AWS S3 console to upload the edited current file to the bucket transitanalystisrael-current/docs
d.5. use the AWS S3 console to upload the edited past file to the bucket transitanalystisrael-past/docs


1.2 - On-demand files
---------------------
A bucket will be created to host the files on the S3 service with a unique name as appears in the config file.
a.  Open TransitAnalystIsrael/root bucket_prefix transitisrael_config.py
	The name will be made according to the combination of the following parameters
	bucket_prefix+gtfsdate e.g. transitanalystisrael-20190307

b. Updating or Valdiating Transit Analyst Israel files are up-to-date with the EC2 API Gateway URL
The static HTML files should have the correct API Getway URL that points to the AWS EC2 before being uplaoded to the S3.
a. Go to https://eu-central-1.console.aws.amazon.com/apigateway/
b.1. Click on the API Gateway you defined earlier -> Click "Stages" -> Click on the staging name -> Copy the invoke URL
b.2. Make sure that that parameter time_map_server_aws_url value is the same as the copied invoke URL in the following locations on your local PC:
	 - TransitAnalystIsrael/root/transitisrael_config.py
	 - TransitAnalystIsrael/website_current/transitisrael_config.js
	 - TransitAnalystIsrael/website_past/transitisrael_config.js

c. Performing the upload
c.1. Go to TransitAnalystIsrael/root and run upload_ondemand2aws_s3.py
c.2. Once finished, the files should be available at https://s3.console.aws.amazon.com/s3/buckets/<bucket_prefix+gtfsdate>/

d. Update and upload to S3 config files to make the TTM client look for the TTM server on EC2 and not on the local PC. We do this by changing the config to look like the product processed is on EC2 and S3.
d.1. copy website_yyyymmdd\docs\transitanalystisrael_config.js to a temp folder e.g. C:\temp\yyyymmdd
d.2. edit transitanalystisrael_config.js in the temp folder. 
		Replace the following 6 lines:
// #On demand date on AWS EC2 and S3
// #get_service_date = 'on_demand'
// #python_processing = 'aws_ec2'
// #ttm_graph_processing = 'aws_ec2'
// #web_client_hosted_on = 'aws_s3'
// #ttm_server_on = 'aws_ec2'

		With the following 6 lines:
// #On demand date on AWS EC2 and S3
var cfg_get_service_date = 'on_demand' ;
var cfg_python_processing = 'aws_ec2' ;
var cfg_ttm_graph_processing = 'aws_ec2' ;
var cfg_web_client_hosted_on = 'aws_s3' ;
var cfg_ttm_server_on = 'aws_ec2' ;

		Replace the following 6 lines:
// #On demand date on local pc
var cfg_get_service_date = 'on_demand' ;
var cfg_python_processing = 'local_pc' ;
var cfg_ttm_graph_processing = 'local_pc' ;
var cfg_web_client_hosted_on = 'local_pc' ;
var cfg_ttm_server_on = 'local_pc' ;

		With the following 6 lines:
// #On demand date on local pc
// #get_service_date = 'on_demand'
// #python_processing = 'local_pc'
// #ttm_graph_processing = 'local_pc'
// #web_client_hosted_on = 'local_pc'
// #ttm_server_on = 'local_pc'

d.3. use the AWS S3 console to upload the edited config file to the bucket transitanalystisrael-yyymmdd/docs

2. Upload generated Navitia graph to AWS EC2
============================================
a. Make sure that one of the Navitia docker containers is running by performing step 4 in "Instructions for On demand date on local pc.txt"

2.1 - Monthly update graph
--------------------------
a. Transfer the generted Navitia graph and the secondary graph from the local docker container` to your local pc file system:
a.1. Open Windows power shell and run the following command to copy the graphs to <some-dest-folder> on your local machine e.g. "C:\temp":
$  docker cp navitia-docker-compose_tyr_worker_1:/srv/ed/output/default.nav.lz4 C:\temp
$  docker cp navitia-docker-compose_tyr_worker_1:/srv/ed/output/secondary-cov.nav.lz4 C:\temp
a.2. Copying the graphs to your AWS EC2 machine:
	In order to copy the files from your machine to the EC2, you need the .pem key that allows you to access the EC2 (you got the .pem key when you created the EC2 instance - probably in the s3 key bucket). 
	a. Place both the graphs and the key in the same folder.
	b. Right-click in the widnows explorer -> Open "Git Bash" here
	c. The copy command requries the following information:
	   i. the name of the .pem file <pem-file>.pem, e.g. key.pem
	   ii. name of the grpah file: default.nav.lz4 or secondary-cov.nav.lz4
	   iii. user name that you use to connect to the EC2 with putty <user-name>, e.g. ubuntu
	   iv. public DNS of the EC2 instace that can be found at the AWS EC2 console (http://console.aws.amazon.com/ec2) -> in the EC2 instances list -> column "Public DNS (IPv4)"), <public-dns>, e.g. ec2-3-122-15-201.eu-central-1.compute.amazonaws.com
	d. Type the command: "scp -i <pem-file>.pem default.nav.lz4 <user-name>@<public-dns>:/home/ubuntu/" e.g.
$ scp -i transitanalystisrael.pem default.nav.lz4 ubuntu@ec2-3-122-15-201.eu-central-1.compute.amazonaws.com:/home/ubuntu/
	e. Type the command: "scp -i <pem-file>.pem secondary-cov.nav.lz4 <user-name>@<public-dns>:/home/ubuntu/" e.g.
$ scp -i transitanalystisrael.pem secondary-cov.nav.lz4 ubuntu@ec2-3-122-15-201.eu-central-1.compute.amazonaws.com:/home/ubuntu/

a.3. Delete the default.nav.lz4 and secondary-cov.nav.lz4 file from your local PC to save storage (it's still avialible inside the docker container)

b. Copy the graphs into the docker container on the EC2 machine:
b.1. Using putty, connect to the EC2 machine and perform the following commands to delete the graphs from the docker 
	 - connect to the docker worker container:
$ docker exec -i -t navitia-docker-compose_tyr_worker_1 /bin/bash
	 - Change directory to the graphs folder:
root@9bb282f314b6:/usr/src/app# cd /srv/ed/output
	 - Delete the secondary-cov.nav.lz4 graph:
$ rm secondary-cov.nav.lz4
	 - Delete the default.nav.lz4 graph:
$ rm default.nav.lz4
	-  Exit the internal docker terminal:
root@9bb282f314b6:/srv/ed/output#  exit

b.2. Copy the new graphs into the docker container (continue using putty):
$ docker cp default.nav.lz4 navitia-docker-compose_tyr_worker_1:/srv/ed/output/
$ docker cp secondary-cov.nav.lz4 navitia-docker-compose_tyr_worker_1:/srv/ed/output/

c. Stop and Re-start Navitia containers:	
	a. Go to the folder where you cloned the navitia-docker-compose repo e.g.:
$ cd ~/navitia-docker-compose
	b. Stop the running navitia dockers:
$ docker stop $(docker ps -a -q)
	d. Once done - run Navitia container with the secondary-cov: 
$ sudo docker-compose -f compose_files/docker-compose-secondary-cov.yml -p navitia-docker-compose up  --remove-orphans


Once docker is up, you can close the putty terminal and navigiate to the website_current index.html file on the S3 to validate that everything works.

2.2 - On-demand graph
---------------------
When the on-demand process is finished a graph named ondemand-YYYYMMDD.nav.lz4 is generted, e.g. ondemand-20190304.nav.lz4 (where YYYYMMDD is the date you put for the gtfsdate). 
a. Transfer the generted Navitia graph from the docker container to your local pc:
a.1. Open Windows power shell and run the following command to copy the graph to <some-dest-folder> on your local machine e.g. "C:\":
$  docker cp navitia-docker-compose_tyr_worker_1:/srv/ed/output/ondemand-20190304.nav.lz4 <some-dest-folder>
a.2. Copying the graph to your AWS EC2 machine:
	In order to copy the file from your machine to the EC2, you need the .pem key that allows you to access the EC2 (you got the .pem key when you created the EC2 instance - probably in the s3 key bucket). 
	a. Place both the graph and the key in the same folder.
	b. Right-click in the widnows explorer -> Open "Git Bash" here
	c. The copy command requries the following infromation:
	   i. the name of the .pem file <pem-file>.pem, e.g. key.pem
	   ii. name of the grpah file: ondemand-20190304.nav.lz4
	   iii. user name that you use to connect to the EC2 with putty <user-name>, e.g. ubuntu
	   iv. public DNS of the EC2 instace that can be found at the AWS EC2 console (http://console.aws.amazon.com/ec2) -> in the EC2 instances list -> column "Public DNS (IPv4)"), <public-dns>, e.g. ec2-3-122-15-201.eu-central-1.compute.amazonaws.com
	d. Type the command: "scp -i <pem-file>.pem ondemand-20190304.nav.lz4 <user-name>@<public-dns>:/home/ubuntu/" e.g.
$ scp -i key.pem ondemand-20190304.nav.lz4 ubuntu@ec2-3-122-15-201.eu-central-1.compute.amazonaws.com:/home/ubuntu/

a.3. Delete the ondemand-20190304.nav.lz4 file from your local PC to save storage (it's still avialible inside the docker container)

b.1. Using putty, connect to the EC2 machine and perform the following commands to copy the on-demand graph to be the on-demand graph.     
b.2. Copy the new on-demand graph into the docker container:
$ docker cp ondemand-20190304.nav.lz4 navitia-docker-compose_tyr_worker_1:/srv/ed/output/

c. Stop and Re-start Navitia servers:	
	a. Go to the folder where you cloned the Navitia-docker-compose repo e.g.:
$ cd ~/navitia-docker-compose
	b. Stop the running navitia dockers:
$ docker stop $(docker ps -a -q)
	d. Once done - run Navitia server with the secondary-cov: 
sudo docker-compose -f compose_files/docker-compose-ondemand-YYYYMMDD.yml -p navitia-docker-compose up --remove-orphans (where YYYYMMDD is the date you put for the gtfsdate)


Once docker is up, you can close the putty terminal and navigiate to the on-demand index.html file on the S3 to validate that everything works.